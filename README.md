# End-to-End Data Engineering Pipeline on Azure (Brazilian E-Commerce Dataset)

## ğŸ“Œ Project Overview
This project demonstrates a complete **data engineering pipeline** built on **Microsoft Azure** using the [Brazilian E-Commerce Public Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).  
The solution is designed with the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) to ensure scalable, reliable, and analytics-ready data processing.

---

## ğŸš€ Architecture Workflow

1. **Data Ingestion**  
   - Raw CSV files hosted on **GitHub** and a subset stored in a **SQL Database**.  
   - Used **Azure Data Factory (ADF)** pipelines to ingest data from:
     - GitHub (via HTTP connector).  
     - SQL Database (via JDBC connection).  
   - Landed all raw data into **Azure Data Lake Storage Gen2 (ADLS Gen2)** under the **Bronze layer**.

2. **Data Transformation & Enrichment**  
   - Leveraged **Azure Databricks (Spark)** for cleaning, joining, and transforming datasets.  
   - Added external enrichment data from **MongoDB**, ingested via Spark into **Bronze**.  
   - Produced structured and cleaned **Parquet files** stored in the **Silver layer**.

3. **Serving Layer**  
   - Used **Azure Synapse Analytics (Serverless SQL Pool)** to create external tables and final curated **views** from Silver data.  
   - Exposed business-ready datasets in the **Gold layer** for downstream analytics and visualization.

4. **Visualization / Consumption**  
   - The Gold layer datasets are ready for **BI dashboards** (Power BI, Tableau) or **Data Science workloads**.

---

## ğŸ—ï¸ Azure Resources Used
- **Azure Data Factory (ADF)** â†’ Data ingestion pipelines.  
- **Azure Data Lake Storage Gen2 (ADLS)** â†’ Bronze, Silver, Gold layers.  
- **Azure Databricks** â†’ Data transformation and enrichment.  
- **MongoDB (External)** â†’ Enrichment dataset.  
- **Azure Synapse Analytics** â†’ Serverless SQL pool for querying and serving.  

---

## ğŸ“‚ Medallion Architecture

adls_gen2/
â”‚â”€â”€ bronze/ â†’ Raw ingested data (CSV, JSON, MongoDB extracts)
â”‚â”€â”€ silver/ â†’ Cleaned, transformed, joined data (Parquet)
â”‚â”€â”€ gold/ â†’ Curated data served via Synapse (Views / Tables)


---

## ğŸ”„ Data Pipeline Flow

1. **Ingestion (ADF)**  
   - Pull raw CSV from GitHub.  
   - Load SQL DB tables.  
   - Store in **ADLS â†’ Bronze**.  

2. **Transformation (Databricks)**  
   - Clean missing values, normalize columns.  
   - Join datasets (orders, payments, products, reviews, etc.).  
   - Enrich with MongoDB data.  
   - Store as **Parquet in Silver**.  

3. **Serving (Synapse)**  
   - Create external tables/views on Silver Parquet files.  
   - Expose as **Gold layer** for analytics.  

---

## ğŸ“¸ Project Screenshots

1. **Azure Resources Overview**  
   *(_Insert screenshot of Azure portal homepage showing resources_)*  

2. **Medallion Architecture (Bronze, Silver, Gold Folders)**  
   *(_Insert screenshot of ADLS folder structure_)*  

3. **ADF Pipeline Workflow**  
   *(_Insert screenshot of ADF pipeline workflow_)*  

4. **Synapse Analytics Queries & Views**  
   *(_Insert screenshot of Synapse serving Gold layer_)*  

---

## ğŸ“Š Key Learnings
- Implemented the **Medallion Architecture** with Azure.  
- Built an **orchestrated ETL pipeline** using ADF + Databricks.  
- Designed a **scalable serving layer** with Synapse.  
- Practiced **cloud-native data engineering workflows** end-to-end.

---

## ğŸ› ï¸ Tech Stack
- **Azure Data Factory**  
- **Azure Data Lake Storage Gen2**  
- **Azure Databricks (Apache Spark)**  
- **Azure Synapse Analytics**  
- **MongoDB (Enrichment Data)**  
- **Python, SQL**  

---

## ğŸ“Œ Next Steps (Future Enhancements)
- Automate pipeline scheduling with **ADF triggers**.  
- Integrate **Power BI dashboards** directly with Gold layer.  
- Add **CI/CD deployment** for Databricks notebooks and ADF pipelines.  

---

## ğŸ“§ Contact
For questions or collaboration, feel free to connect!  


